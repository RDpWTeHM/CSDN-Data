# v0.0.4 stablize and applize

  这一版使运行更稳定和更加“应用化”。

增加了 **配置文件** 方案。使用 python 作为配置文件。

## *Overview*

[TOC]

## Contents

```
v0.0.4 stablize and applize
    Overview
    Contents
    Summary
    TODO
    Note
    Change Log(2018)
```



## Summary

```shell
CSDN-Data$ cd ./tmp && ./shelveDBInfo.py && cd ../
#### 检查 shevle DB 的状态
## ALL_USER_IDs 的数量应当要大于 loop articles client 中需要的数量
# example run result:
length of ALL_USER_IDs:  152
length of left_ALL_USER_IDs:  5775
length of TIMEOUT_IDs:  2329
length of DONE_IDs:  1835
CSDN-Data$ ./tmp/migrate_test_UserID_DB.py
#### 从 left_ALL_USER_IDs 中移动 100 个 user id 到 ALL_USER_IDs 中。
CSDN-Data$ ./loopCSDNCrawlerArticles.py
## 开始爬取数据
## 如何 logging 的 basicConfig 开启了 log 到文件中，
## 则可以直接加 "*.py &" 后台运行。因为代码中的输出都会导入到 log 文件中。
```



## TODO

- [ ] 不足分页的 User ID 获取 articles 出现错误的 timeout 的 issue。
- [x] logging 模块 - 记录 Log。
- [ ] loop articles client 的 shelve db 锁机制测试。
- [ ] loop articles client 的 `Qid_in` 加入新的 user id 任务机制。

## Note

n/a


## Change Log

### Nov/25

#### 调整目录结构

将 `CSDNVisualize/CSDNCrawler/libCrawler` --移动到--> `CSDNVisualize/libCrawler`

#### merge Crawler>CSDN v0.0.7

  Crawler CSDN v0.0.7 不算完全完成，但是也有解决了一个比较重要的 bug，已经对代码结构做了一些更新。

fix 了 handle page source 的 daemon IPC 链接建立时 refuse issue。

将获取特定 site 的 page source 的代码分离到独立的文件中去。

#### 18:49 configuration files

使用 `*.py` 作为 configuration file，import 它，和使用 `nametuple`。



#### 20:00 测试获取 articles 的问题

出现了一个浏览器失去控制的情况！

- [x] 4x10 虽然一个浏览器失去控制，但是和获取多少个 User ID 没有关系，却只有 30 的 User ID 被请求。

  其中有一个接近 100 个分页的 User ID，在这个请求完了之后，又能够请求2个。但是请求了两个之后，又完全卡死了。

  -- 在下面代码的更新之后，虽然还是有些不稳定，但是问题已经不在这里了。

- [x] 这一次测试完成，看 ALL_USER_IDs 中的数据是否正确 - 代码是否按逻辑工作？

  -- 更新 shelve db 数据不正确，不过最后在下面的开发中，使用一个独立线程处理 result 正确更新了 shelve DB。

- [ ] 一个失去控制的浏览器，在最后被 *自动* 退出了 -- 不是我关闭的。

------

- [x] 取了 all user id 中的随机 100 个 User ID，修改 client loop 代码，测试 4x5 个看能不能成功完成。

  -- 因为 hang 住的位置不清楚，所以比较难办。

在上面并不成功地 loop 三十几个 UserID 之后，不重启 handle page source daemon，工作还算正常。

- [x] 错误的 timeout 情况也比较多（有文章，但是不足分页的 UserID 是否有“下一页”？）

  -- 预计应该是因为这个原因，所以出现很多错误的 timeout 的情况。

  （预期 timeout 只用于 0原创 0 转发的 User ID）

最新的观察出来，client loop 多个线程没有并发地执行！遇到分页很多的 User ID 也就是会长时间运行的线程，其它线程完成了一些之后，就停止了，然后等那个长时间线程完成之后，可以再运行一两下，这是为什么？？？？？？

-- 看了一下代码，是因为线程是使用 for 循环的方式，所以一个线程从 Queue 中取了 for 循环的次数之后，就停止了，这样导致了某些特定（运气不好，总是拿到 User ID 的 articles 比较多）的线程运行时间过长，程序的清扫工作太迟！

- [x] 重新使用和 handler page source daemon 一样的线程池方式！

  -- 固定数量的 missions 放到 Queue 中，最后完成了，就放入 'Stop' mission，线程自己做清理工作。

> 第三次也是 4x5 个 User IDs，虽然最开始死掉一个 browser，但是目前运行了这几次来看，还算稳定。
>
> 这和原本的情况一致，原先也是可以运行几个小时。后来才死掉了程序。

- [x] connection refuse 问题看来没有完全 merge 好，需要在 `CSDN-Data` 上进一步 fix。

  -- 原因是因为原本只 retry 了 utils.py 中获取 page source 的 VisualData 部分，而 获取 page sources 的 Articles 没有 retry；现在已经 fixed。

#### Nov/26 01:18 loop articels

使用线程池和 Queue 的方式，而不是一个线程固定做多少次请求。

> 实际上这里是我理解错误，原本这里就是为了测试，临时使用的 for 循环，实际上，它本来就是 while 线程池的形式去取 Queue 中的任务。

增加了一个 handler result 线程来更新 shelve db，和一个 handler error 线程来更新 log 文件。

最后测试的时候还是出现了问题，有一个页面加载了一次，然后 hang 住了，不知道浏览器 hang 住，daemon handler page source 中的代码还有没有怎么运行。但是这个地方 hang 住之后，client 这条线程也就 hang 住了。



本次代码由于多了一条线程实时 update shelve DB，所以既是程序 hang 住，强制中断退出，shelve DB 数据也算比较准确！

所以这一个更新还算比较成功！

------



- [ ] 更新 shelve DB 的线程锁
- [ ] article client active 的 Ctrl+C SIGTERM 信号机制。



### Nov/26

#### 11:37

早上测试 30 个 User ID，又出现了一个浏览器死掉的情况。

然后导致有一个 client 线程无法退出！

这个 bug 需要解决，不过和上面说的一样，结果还可以接受。



#### 13:52 浏览器失去控制

handler page source 的 daemon 程序增加一个 manage browsers 的线程，再增加一个可以监控状态的线程！

14:20 先简单调整了一下 daemonize_use_threadpool.py 文件中的部分代码，移动到了新文件 `threadpool.py` 中去。

14:53 使用了 `stop_and_free_thread_pool()` 函数，将其放在 handler SIGTERM 的函数内部，解决了原本需要多次 stop 才能退出的问题。



19:48 浏览器异常失去控制的问题，忽然想起来应该是因为“下一页”被加载出来之后，但是实际上 js 代码没有完全能工作，导致 selenium 点击了下一页，但是此时已经没有了超时机制，而页面也停止了加载，所以相当于 hang 住了。时间过长就会有失去控制退出的情况。

增加一些判断机制在 articles 的下一页点击操作中。

22:39 简单增加了 url 是否相同的判断。

使用 6 个浏览器开始测试 -- 主要测试这六个浏览器中是否会有浏览器失去控制。

23:26 在树莓派3B 上，100 个 User ID 需要跑超过 1h。

速度不快。对于 404 这类页面和文章数量不足没有分页的，可以加快一下速度。

跑了 60+ 个 ID，没有出现问题，今天白天在公司，第一次跑 60 个 User ID 就出现了一个浏览器 down 掉。

所以应该确实是这个问题，而且现在解掉了。



总的来说，这个 bug 的解决令我发现了一个比较有意思的地方，解法是判断“点击下一页”之后，浏览器当前的 url 是否切换，如果没有切换，说明“下一页”不可用（这个 blog 页面的 “下一页”并非 url，而是 js 脚本执行）。也就是原本没有真正加载完全。但是下一页的那个判断是否 timeout 的元素却加载出来了。导致了 timeout 无效。所以浏览器永远 hang 住，最后被一些可能是 selenium 还是 django 的调用什么机制给关闭掉了（selenuim 的可能性比较大，因为浏览器关闭了，django 也是很有可能的）。

但是之前在一个设计判断是否“没有”下一页，到了尾页，特地花了一些测试局部代码来实现。而当时却没有想到用如果“下一页”点击有效，url 自然会变掉，可以通过这个来判断是否到了尾页。

我的意思是，选择了上述的方案来实现判断是否所有页面都被 save（加载到了最后一页），然后退出该账号的信息获取。这样的话，上述说的 bug 也就不会出现了。一个功能选择了另外一种实现方式（判断条件）就能避免掉一些 bug！这不仅仅是运气问题。

所以在有多种解决方案的时候，是可以严谨地把几种方案拿出来显式地比较的。并且要多留心隐藏的 bug。

<p align="right">2018/Nov/26 23:39</p>



- [ ] 很不幸，今天晚上 stop 退出 daemon handler page source 进程 -- 仍然需要两次才能退出。



### Nov/27

#### 10:20

测试浏览器失去控制问题。

目前没有出现失去控制的情况！

但是出现了一个新的 bug：

```shell
userid:  m_changgong
[Debug]: negotiateIPCInfo> Client received:  {'port': 16267}
[27/Nov/2018 10:02:54] "GET /CSDNCrawler/loopArticles/?user_id=leverage_1229 HTTP/1.1" 200 11
[27/Nov/2018 10:03:11] "GET /CSDNCrawler/loopArticles/?user_id=xiamiwage HTTP/1.1" 200 11
Internal Server Error: /CSDNCrawler/loopArticles/
Traceback (most recent call last):
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/backends/utils.py", line 85, in _execute
    return self.cursor.execute(sql, params)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/backends/sqlite3/base.py", line 296, in execute
    return Database.Cursor.execute(self, query, params)
sqlite3.OperationalError: database is locked

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/core/handlers/exception.py", line 34, in inner
    response = get_response(request)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/core/handlers/base.py", line 126, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/core/handlers/base.py", line 124, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "/home/joseph/Devl/SVN/CSDN-Data/CSDNVisualize/CSDNCrawler/views.py", line 217, in loopArticles
    newArticlesData.save()
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/models/base.py", line 718, in save
    force_update=force_update, update_fields=update_fields)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/models/base.py", line 748, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/models/base.py", line 831, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/models/base.py", line 869, in _do_insert
    using=using, raw=raw)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/models/manager.py", line 82, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/models/query.py", line 1136, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/models/sql/compiler.py", line 1289, in execute_sql
    cursor.execute(sql, params)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/backends/utils.py", line 100, in execute
    return super().execute(sql, params)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/backends/utils.py", line 68, in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/backends/utils.py", line 77, in _execute_with_wrappers
    return executor(sql, params, many, context)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/backends/utils.py", line 85, in _execute
    return self.cursor.execute(sql, params)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/utils.py", line 89, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/backends/utils.py", line 85, in _execute
    return self.cursor.execute(sql, params)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/db/backends/sqlite3/base.py", line 296, in execute
    return Database.Cursor.execute(self, query, params)
django.db.utils.OperationalError: database is locked
[27/Nov/2018 10:06:05] "GET /CSDNCrawler/loopArticles/?user_id=byxdaz HTTP/1.1" 500 153429
[27/Nov/2018 10:06:06] "GET /CSDNCrawler/loopArticles/?user_id=dqcfkyqdxym3f8rb0 HTTP/1.1" 200 11
```

-- 不过这个 bug 不太算我写的代码问题，所以目前还算能接受。



#### 12:04 仍然失去控制

问题变得比较难办。

早上确实算是运行了较久的时间。最后 六个浏览器没有关闭，但是失去了控制？？？？

中断了 client 之后，client 重新获取 articles，这个时候失败。不能再控制任何浏览器。

django 错误信息：

```shell
userid:  hgy413
Internal Server Error: /CSDNCrawler/loopArticles/
Traceback (most recent call last):
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/core/handlers/exception.py", line 34, in inner
    response = get_response(request)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/core/handlers/base.py", line 126, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "/home/joseph/.virtualenvs/CSDN-Data/lib/python3.5/site-packages/django/core/handlers/base.py", line 124, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "/home/joseph/Devl/SVN/CSDN-Data/CSDNVisualize/CSDNCrawler/views.py", line 201, in loopArticles
    csdn_userid_articles.syncArticlesData()
  File "/home/joseph/Devl/SVN/CSDN-Data/CSDNVisualize/libCrawler/PersonalData/blogpage.py", line 362, in syncArticlesData
    self.getBlogPageHTMLsText()
  File "/home/joseph/Devl/SVN/CSDN-Data/CSDNVisualize/libCrawler/PersonalData/blogpage.py", line 319, in getBlogPageHTMLsText
    data = negotiateIPCInfo()
  File "/home/joseph/Devl/SVN/CSDN-Data/CSDNVisualize/libCrawler/PersonalData/utils.py", line 86, in negotiateIPCInfo
    data = sockObj.recv(1024)
ConnectionResetError: [Errno 104] Connection reset by peer
[27/Nov/2018 12:03:11] "GET /CSDNCrawler/loopArticles/?user_id=hgy413 HTTP/1.1" 500 78677
```

##### can't start new thread

> fixed on [22:32 fix start new thread](#22:32 fix start new thread)

daemon 记录的错误 log：

```shell
joseph@jos-RPi3Bplus:daemonize_use_threadpool$ cat error_20181127_0926.log 
Exception in thread Thread-49:
Traceback (most recent call last):
  File "/usr/lib/python3.5/threading.py", line 914, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.5/threading.py", line 862, in run
    self._target(*self._args, **self._kwargs)
  File "./spider/daemonize_use_threadpool.py", line 302, in handleClient
    lock=_lock)
  File "./spider/daemonize_use_threadpool.py", line 270, in server_handler
    serv = Listener(address, authkey=authkey)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 438, in __init__
    self._listener = SocketListener(address, family, backlog)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 576, in __init__
    self._socket.bind(address)
OSError: [Errno 98] Address already in use

Traceback (most recent call last):
  File "./spider/daemonize_use_threadpool.py", line 420, in <module>
    main(len(sys.argv), sys.argv)
  File "./spider/daemonize_use_threadpool.py", line 317, in main
    dispatcher()
  File "./spider/daemonize_use_threadpool.py", line 316, in dispatcher
    newIPCHandlerThread.start()
  File "/usr/lib/python3.5/threading.py", line 844, in start
    _start_new_thread(self._bootstrap, ())
RuntimeError: can't start new thread
[Error]: in commad='CSDN-Data' handler: 
[Error]: in commad='CSDN-Data' handler: 
[Error]: in commad='CSDN-Data' handler: 
[Error]: in commad='CSDN-Data' handler: 
joseph@jos-RPi3Bplus:daemonize_use_threadpool$
```

虽然问题比较难办，但是 daemon 的使用 “stop” 终止程序这一次使完全 work 的情况。

浏览器全部退出，进程不需要两次 stop，一次 stop 就退出干净了。



------



### Nov/28

#### 15:13 浏览器仍然失去控制

现在是一次性多个浏览器失去控制，而且进程发生在 loop-client 快要结束的时候，其中一个 loop 线程已经关闭了

```
loop 'pony1001' articles: <h1>OK</h1>
loop 'intleisure' articles: <h1>Timeout</h1>
loop 'fanzheng220112583' articles: <h1>OK</h1>
[Info] queue.Empty! I am quit!
run stop to quit thread
[Debug] handler_result(): command is None
^CTraceback (most recent call last):
  File "./loopCSDNCrawlerArticles.py", line 228, in <module>
    main()
  File "./loopCSDNCrawlerArticles.py", line 220, in main
    existing_thread.join()
  File "/usr/lib/python3.5/threading.py", line 1054, in join
    self._wait_for_tstate_lock()
  File "/usr/lib/python3.5/threading.py", line 1070, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt
(CSDN-Data) joseph@jos-RPi3Bplus:CSDN-Data$ 
```

但是这个时候，却 hang 住了。

此时重新运行 loop-client，django 的 client 请求完全不能和 daemon server 正常工作。我的意思是，不知道 django 那边有没有发送 socket 和 IPC，现象就是 四个浏览器完全没有反应。



### Nov/29

#### 10:57 浏览器失去控制

这次至少有一些信息可以参考了。

首先早上使用的是两个浏览器，两个线程并行请求，和处理请求。

一共 300 个 user ids；

在 234 个之后，浏览器没有反应了。

但是因为这次浏览器开得较少，所以看了 log 之后，发现“不是控制浏览器”的问题。

使用当前的停止命令 `./spider/daemonize_[TAB] stop` 来停止 daemon 进程。

注意到， loop Articles 的 client 马上可以继续不断地执行。

说明说，django crawler page source client 和 daemon handler page source server 之间断开了，并且无法建立新的 socket 链接（先 socket，再 IPC），于是 django 回复请求应该是 500 这样的。

(当前的代码，loop Articels client 收到 500 直接 raise 空的 Exception)

在这种情况下，loop Articles client 能够不断处理，反应没有阻塞。



那么，现在来看看 daemon “不是控制浏览器”问题，看看 log 到底是什么问题：

```shell
Exception in thread Thread-124:
Traceback (most recent call last):
  File "/usr/lib/python3.5/threading.py", line 914, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.5/threading.py", line 862, in run
    self._target(*self._args, **self._kwargs)
  File "./spider/daemonize_use_threadpool.py", line 309, in handleClient
    lock=_lock)
  File "./spider/daemonize_use_threadpool.py", line 277, in server_handler
    serv = Listener(address, authkey=authkey)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 438, in __init__
    self._listener = SocketListener(address, family, backlog)
  File "/usr/lib/python3.5/multiprocessing/connection.py", line 576, in __init__
    self._socket.bind(address)
OSError: [Errno 98] Address already in use
```

看来问题和之前一样，上一次的修复改错了位置。

```python
    def server_handler(address, authkey, lock=None):
        _lock = lock
        if _lock is not None:[...]
        else:
            try:
                serv = Listener(address, authkey=authkey)
            except OSError as e:
                # OSError: [Errno 98] Address already in use
                # -[o] joseph, log error here
                sys.stderr.write(
                    "[WARNING] OSError: {}; and pass this.\n".format(e))
                # pass this connection as server refuse-that client retry fail
                pass
            except Exception:
                traceback.print_exc()
            while True:
                try:
                    client_conn = serv.accept()

                    client_IPC_handler(client_conn)
                except Exception:
                    traceback.print_exc()
```

仍然是 300 个，现在可以再测试一遍！

上面 solution 错误，OSError 之后不能 pass，不然下面的 while True 代码会运行，然后 `serv` 相当于没有复制。是空的使用。

这个问题暴露了写 Python 已经变得不敢随便再函数中使用 return 了。

另一个问题就是 `except Exception: traceback.print_exc()` 同样有这个问题。说明对 `traceback.print_exc()` 的理解不正确，在 while True 中使用使这个问题变得十分严重。

**该 return 就 return，再测试一遍**



#### 21:40 新的浏览器失去控制原因

17:00+ 左右，下班前注意到 loop articles 还是 hang 住了。不过这一次和之前的原因有所不同。

具体原因还需要分析，相对比较空洞的 daemon log 是这样的：

```shell
[WARNING] OSError: [Errno 98] Address already in use; and pass this.
[Error]: in commad='CSDN-Data' handler: 
[Error]: in commad='CSDN-Data' handler: 
Exception ignored in: <module 'threading' from '/usr/lib/python3.5/threading.py'>
Traceback (most recent call last):
  File "/usr/lib/python3.5/threading.py", line 1288, in _shutdown
    t.join()
  File "/usr/lib/python3.5/threading.py", line 1054, in join
    self._wait_for_tstate_lock()
  File "/usr/lib/python3.5/threading.py", line 1070, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
  File "./spider/daemonize_use_threadpool.py", line 427, in sigterm_handler
    sys.stdout.write("daemon end time: {}\n".format(ctime()))
  File "./spider/daemonize_use_threadpool.py", line 427, in sigterm_handler
    sys.stdout.write("daemon end time: {}\n".format(ctime()))
RuntimeError

```

后面的可能是运行 stop 后出现的。

应该是上面 line 2-3 的 Error 导致浏览器再次失去控制。

至少之前解决的问题还算有效。

继续运行，等分析代码找到原因了 fix。



### Nov/30

#### 17:21 daemon stop 退出

因为下班时间要到了，所以在 client loop aritcles 任务还没有完成就 stop 掉 daemon。

注意到了 stop 这个功能的 bug。对于暂时已经停止请求页面（block 的 thread），stop 确实可以使其退出。但是处于任务中的 thread 在任务完成之后，还是没有按预期的立马退出。

所以这种情况下，比如，现在三个浏览器请求的 user blog 分页数量不同，是一个接一个前后相差很长时间完成的。

那么就需要执行三/四次 stop 才能完全退出程序。

至少需要多次执行 stop 的原因找到了。



### Dec/01

#### 12:20 client loop active issue

在 django 还没有启动的时候，如果先启动 loop articles client 的话，会“疯狂”打印 log

```shell
[Debug] request: crawler articles of "norains"
[Debug] request: crawler articles of "oldmtn"
[Error] <active@norains> unknow condition: HTTPConnectionPool(host='localhost', port=8010): Max retries exceeded with url: /CSDNCrawler/loopArticles/?user_id=norains (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7604e910>: Failed to establish a new connection: [Errno 111] Connection refused',))
[Debug] request: crawler articles of "xinguimeng"
[Error] <active@oldmtn> unknow condition: HTTPConnectionPool(host='localhost', port=8010): Max retries exceeded with url: /CSDNCrawler/loopArticles/?user_id=oldmtn (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7604edf0>: Failed to establish a new connection: [Errno 111] Connection refused',))
[Debug] request: crawler articles of "youkuxiaobin"
[Error] <active@xinguimeng> unknow condition: HTTPConnectionPool(host='localhost', port=8010): Max retries exceeded with url: /CSDNCrawler/loopArticles/?user_id=xinguimeng (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76067290>: Failed to establish a new connection: [Errno 111] Connection refused',))

```

这种 case 不能放在 Exception 中不管，不然全部都是 error log 会导致 log 文件非常大。

- [x] 这个 case 需要写一个精确的 Exception 处理。

  解决在 [14:27 django server not running issue](#14:27 django server not running issue)

#### 19:07 使用 logging

使用 logging 模块记录 client active crawler articles 运行日志。

效果很不错，使用起来没有太多麻烦，很简单就能使用。

Reference: Python3 cookbook - & 13.11

> 正常情况下都不会有问题，但是仍然要考虑到 logging 配置的输入到 log 文件，
>
> 这个 log 文件有可能放的位置不合适的话，就会有权限问题。



（最近每一次爬 200 个 user ids 的 articles 之后，重启 daemon，还算相当稳定的。）

**出现过一次 chromium crash 的情况，于是“必须”重启 Raspberry Pi 3B, 重启之后工作正常。**



#### 21:01 `[Error]: in commad='CSDN-Data' handler:`

之前又出现过一次。现在可以确定，有一个 daemon 的 bug 发生在：

```shell
(CSDN-Data) joseph@jos-RPi3Bplus:libCrawler$ tail /mnt/banq_16g/CSDN-Data/log//daemonize_use_threadpool/error_20181201_1851.log 
[Sat Dec  1 19:04:46 2018][WARNING] OSError: [Errno 98] Address already in use when listen ('', 17134); and pass this.
[Sat Dec  1 19:31:48 2018][WARNING] OSError: [Errno 98] Address already in use when listen ('', 18122); and pass this.
[Error]: in commad='CSDN-Data' handler: 
(CSDN-Data) joseph@jos-RPi3Bplus:libCrawler$
```

其中，出现了 `[Error]: in commad='CSDN-Data' handler:` 这条捕捉到 Exception 的 Error log，就代表有一个 Browser hang 住了。然后代码不会继续运行，django 会随 daemon 的 hang 而一起 hang，loop user id's articles client 会随 daemon 的 hang 一起 hang 住。



### Dec/02

#### 14:27 django server not running issue

参见 [12:20 client loop active issue](#12:20 client loop active issue) 报告问题。

解决方案：移动等待时间 + 精确捕获该异常。

- 改异常类型：

    ```python
            try:
                r = requests.get(loopArticles_url)
            except ConnectionError as e_conn:
                logging.error("requests.get(url) ConnectionError: %s" % (e_conn))
    ```

> 在还没有确认是那个“精确的” error 时，使用：
> 
> ```python
> try:
>     raise <Exception>
> except Exception as e:
>     import traceback; traceback.print_exc();
> ```

这样就能看到输出的集成 Exception 下的错误类型。

- 移动等待时间：

  ```python
  class Work(object):
      global CONF, Qid_in, Qid_done, Qid_err
      [...]
      _totimes = 0
  
      def runningsleep(self, IsNoServer):
          if IsNoServer is False:
              self._totimes = 0
          elif IsNoServer is True:
              self._totimes += 1
              sleep(self._totimes * CONF.NOSERVER_BASEWAITE)
          else:
              raise Exception("<timeoutsleep> Error parameter gived!")
      [...]
      def run(self, ...):
          [...]
          try:
              r = requests.get(loopArticles_url)
          except ConnectionError as e_conn:
              self.runningsleep(True)
              [...]
          else:
              self.runningsleep(False)
  [...]
  ```

#### 21:27 loop articles client error log

将原本将 error message "echo" 到文件的方式使用 logging.error() 的方式记录。



### Dec/04

#### 16:46 浏览器”崩溃“

问题和之前的应该是一样的（因为之前的就还没有 fix）。

原本最近 loop 200 个 user ids 的 articles 就重启一遍 `spider/daemonize_....py` ，还是很稳定的，已经这样运行了 两三天。

但是 13:24 ~ 16:46 这一遍地时间段上，确实出现了一个浏览器被退出。另一个浏览器没有响应 hang 住的情况。

### Dec/07

#### 一种浏览器崩溃原因 - "博客搬家"

观察到一种如果 blog 主页含有“博客搬家”的提示字段在下方，就会出现 hang 到崩溃退出的情况。 

当然，可能还有其它导致浏览器崩溃退出的原因。



#### 22:32 fix start new thread

bug log：[can't start new thread](#can't start new thread)

错误的代码位置：

```python
def main():
    [...]
    def dispatcher():
        while True:
            [...]
            newIPCHandlerThread = threading.Thread(
                target=handleClient,
                args=(connection, IPCListener_lock, ))
            newIPCHandlerThread.start()
    dispatcher()
## ||
## \/
    def handleClient(connection, _lock):
        [...]
        while True:
            data = connection.recv(1024)
            [...]
            connection.send(pickle.dumps(IPCNegotiate))
            connection.close()
            break
        server_handler(('', IPCNegotiate['port']), authkey=b'CSDN-Data',
                       lock=_lock)
##  ||
##  \/
    def server_handler(address, authkey, lock=None):
        _lock = lock
        if _lock is not None:[...]
        else:
            try:
                serv = Listener(address, authkey=authkey)
            except [...]
            
            while True:
                try:
                    client_conn = serv.accept()

                    client_IPC_handler(client_conn)
                except Exception:
                    traceback.print_exc()
                    return
```

注意到上面代码中的 line 33 使用了 `while True`。实际上再看下一层代码，`client_IPC_handler(client_conn)` 捕获处理的 `EOFError`，即 IPC close.

其捕获之后输出 log info 就函数退出了。此时到了这个 `while True` 中，server_handler 本身就属于 handleClient, 是一个独立的线程。也就说这个线程不会退出。

这样就导致了这个程序中创建的线程越来越多。

在本处的应用中，这个线程是一次性使用的。所以必须要退出：

```python
    def server_handler(address, authkey, lock=None):
        _lock = lock
        if _lock is not None:[...]
        else:
            try:
                serv = Listener(address, authkey=authkey)
            except OSError as e:
                # OSError: [Errno 98] Address already in use
                # -[o] joseph, log error here
                logging.warning("OSError: {} when listen {}; and pass this.\n".format(e, address))
                # pass this connection as server refuse-that client retry fail
                return
            except Exception:
                traceback.print_exc()
                return
            if True:
                '''
                  MUST NOT use while True here,
                  this IPC only established once and quit.
                '''
                try:
                    client_conn = serv.accept()
                    client_IPC_handler(client_conn)
                except Exception:
                    traceback.print_exc()
                    return
```

将 `while True` 删除掉即可。

- [x] 还未测试 -- 15:39 测试中，目前测试的时间不够长，但是预期不会有问题。

#### Dec/08

##### 13:14 daemon 上的 logging 问题

在 handler page source request 的 daemon 上使用 DEBUG 级别的话，会将 `selenium` 库的 log 也输出。所以自己开发的使用 log，至少全部提高一个级别！

开发时使用 `INFO` 级别。功能，运行测试等基本稳定了，使用 `WARNING` 级别记录日志！



中午用三个线程-三个浏览器开始，要用来测试 "can't start new thread" 问题，但是有一个浏览器在一开始就崩溃退出了，和上面说的“博客搬家” 导致“下一页”无法点击的问题不清楚是不是同一个，因为那个时刻没有注意到该浏览器上的内容显示。

- [ ] 页面“停留”超过一定时间 `selenium` 就应该保存浏览器页面截图！等浏览器崩溃退出自然已经来不及了。

##### 13:43

调整了 daemon 进程的 logging level！

在 `spider/sites/csdn.py` 中也只要 `import logging` 即可 logging 写入到和 `from sites import csdn` 的 main *.py 文件中配置的 logging 文件一致。



上述的浏览器退出，确实就是因为 “博客搬家”  不能点击 “下一页” 的问题。

不能点击“下一页” 的 log:

```shell
selenium > browser issue?：\
Message: unknown error: Element \
<li class="js-page-next js-page-action ui-pager">...</li> is not clickable at point (535, 329).\
Other element would receive the click: \
<div style="width:100%; height:12%; position:fixed; bottom:0; left:0px; background-color:#333">...</div>
  (Session info: chrome=70.0.3538.110)
  (Driver info: chromedriver=2.41,platform=Linux 4.4.38-v7+ armv7l
```



##### 13:50 *Workaround* "博客搬家"导致的 “下一页”无法点击问题

log 出行的代码位置：

```python
## 在 daemonize_**.py 中记录 log
## 发生在：
            for nextpage_tag in nextpage_tags:
                if "下一页" in nextpage_tag.text:
                    nextpage_tag.click()
                    break
```

去了解这个异常类有些麻烦，直接在这个代码位置先捕获：

```python
### sites/csdn.py

            try:
                for nextpage_tag in nextpage_tags:
                    if "下一页" in nextpage_tag.text:
                        nextpage_tag.click()
                        break
            except WebDriverException as e:
                logging.error("url: %s click issue: %s" % (_url, e))
                raise DaemonCrawlerClickError
```

牵一发动全身，加了一个捕获异常要这个后台一直传染到了 loop articles 的 client 都要针对这个异常记录处理一下。

主要还是原本对这些异常情况在最开始就没有设计好。这也是有预期的。

针对这个改动，需要对照 version control 的 log - diff 来查看。(r935)

##### 14:58 update workaround

上面添加了 “Crawler Error” 的异常基本上没有问题。

不过忘记了像 “Timeout” 一样处理处理请求的 tab。调整了代码：

```python
#### daemonize_use_threadpool.py

                try:
                    thisThreadBrowser = browserQ.get()  # this may block
                    safe_get_tab = thisThreadBrowser.window_handles[0]
                    [...]
                except TimeoutException:
                    logging.warning("browser timeout")
                    conn.send("timeout")  # timeout case!
                except DaemonCrawlerClickError as err:
                    logging.error("%s @ %s" % (req_data['req_url'], err))
                    conn.send("Crawler Error")
                except WebDriverException as err:
                    logging.error("selenium > browser issue?：{}".format(err))
                    conn.send("Crawler Error")
                except Exception as e:
                    logging.error("in commad='CSDN-Data' handler: %s" % (e))
                    conn.send("Crawler Error")
                finally:
                    thisThreadBrowser.close()
                    thisThreadBrowser.switch_to.window(safe_get_tab)
                    browserQ.put(thisThreadBrowser)
```

这种 close 方法基本上没什么问题。

出现了一个新的导致浏览器退出的情况，

###### 15:20 `finally` 使用需要小心

```
[2018-12-08 15:13:36,410]ERROR    selenium > browser issue?：\
Message: no such window: target window already closed
from unknown error: web view not found
  (Session info: chrome=70.0.3538.110)
  (Driver info: chromedriver=2.41,platform=Linux 4.4.38-v7+ armv7l)

[2018-12-08 15:13:36,415]WARNING  Connection closed!
[2018-12-08 15:13:36,831]ERROR    selenium > browser issue?：Message: invalid session id
  (Driver info: chromedriver=2.41,platform=Linux 4.4.38-v7+ armv7l)

[2018-12-08 15:13:36,832]WARNING  Connection pool is full, discarding connection: 127.0.0.1
[2018-12-08 15:13:36,835]ERROR    <({'whichPage': 'ArticlesPages', 'IsReady': 'Ready', \
'req_url': 'https://blog.csdn.net/cf2suds8x8f0v'}, <multiprocessing.connection.Connection object at 0x7592f230>)>\
@<CSDN-Data>: \
Message: invalid session id
  (Driver info: chromedriver=2.41,platform=Linux 4.4.38-v7+ armv7l)

[2018-12-08 15:13:36,838]WARNING  Connection closed!
[2018-12-08 15:13:36,840]ERROR    <({'whichPage': 'ArticlesPages', 'IsReady': 'Ready', \
'req_url': 'https://blog.csdn.net/easy_we'}, <multiprocessing.connection.Connection object at 0x761aa610>)>\
@<CSDN-Data>: \
Message: invalid session id
  (Driver info: chromedriver=2.41,platform=Linux 4.4.38-v7+ armv7l)

[2018-12-08 15:13:43,548]WARNING  Server Be Connected by ('127.0.0.1', 46938) at Sat Dec  8 15:13:43 2018
```

这是因为上面的调整，将

```python
thisThreadBrowser.close()
thisThreadBrowser.switch_to.window(safe_get_tab)
browserQ.put(thisThreadBrowser)
```

放在了 `finally` 中，但是漏删了正常情况（try:...）中的这三行代码，只删除了 timeout 情况中的。

将其删除即可。



###### 15:37 Summary

目前看来情况良好，本次解了两个 bug，

一是“下一页”无法点击（被“博客搬家”字段覆盖）导致的浏览器退出。

二是 “can't start new thread” issue，虽然目前测试的时间不够长，但是预期不会有问题。



增加了一个新功能， 使用 logging 模块记录运行日志！ -- 情况良好。



loop articles client 增加了捕获 Ctrl+C (SIGINT) 新功能，但是因为当前的该程序 `Qid_in` 状态，所以该功能暂时不可用。



------



### Dec/08

#### 22:22 浏览器无法关闭 tab 崩溃退出

从 18:38 开始运行，3.5h 之后，出现了如下 ERROR, log:

```
[2018-12-08 22:01:01,822]ERROR    <({'req_url': 'https://blog.csdn.net/notbaron', \
'whichPage': 'ArticlesPages', 'IsReady': 'Ready'}, \
<multiprocessing.connection.Connection object at 0x75944290>)>\
@<CSDN-Data>: \
Message: unknown error: failed to close window in 20 seconds
  (Session info: chrome=70.0.3538.110)
  (Driver info: chromedriver=2.41,platform=Linux 4.4.38-v7+ armv7l)

```



- [ ] 这种情况下该线程是什么状态？？？





